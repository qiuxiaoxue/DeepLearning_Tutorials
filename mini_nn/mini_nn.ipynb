{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 不依赖任何第三方库构建一个最简单的神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文件说明\n",
    "* `neuron.py`:：单个神经元类`Neuron`\n",
    "* `neuron_layer.py`：单个神经网络层类`NeuralLayer`，包含多个神经元\n",
    "* `neural_network.py`：神经网络类`NeuralNetwork`，包含多个神经网络层\n",
    "* `runner.py`：运行神经网络的脚本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建思路\n",
    "\n",
    "### 网络结构\n",
    "\n",
    "我们知道一个神经网络由多个网络层构成，一个网络层由多个神经元组成，那么我们就可以设计三个类：神经元类、网络层类和神经网络类，三者呈递进的`is_a`关系。\n",
    "\n",
    "#### 神经元类`Neuron`\n",
    "\n",
    "神经元主要负责对输入数据求加权和并施加激活函数，实现非线性变换。\n",
    "\n",
    "神经元类`Neuron`包含的属性：\n",
    "\n",
    "* 输入元素个数属性\n",
    "* 权值属性\n",
    "* 偏移值属性\n",
    "* 激活函数属性\n",
    "* 激活函数导数属性（用于求梯度）\n",
    "\n",
    "方法：\n",
    "\n",
    "* 通过输入值求加权和方法\n",
    "* 通过加权和求输出值（激活函数输出值）方法\n",
    "\n",
    "静态方法：\n",
    "\n",
    "常用激活函数及其导数\n",
    "\n",
    "#### 网络层类`NeuronLayer`\n",
    "\n",
    "一个网络层由多个神经元组成，网络层负责调用通过神经元的求值函数求得当前层的输出，即前向`feed_forward`过程，以及后向的反向传播`back_propagation`过程（即求梯度）。\n",
    "\n",
    "每个网络层保存上一层至这一层的权重参数，这样n层的网络实际需要n-1个`NeuronLayer`对象，因为输入层不需要创建一个`NeuronLayer`对象。\n",
    "\n",
    "属性：\n",
    "\n",
    "* 权值属性\n",
    "* 偏移属性\n",
    "* 组成的神经元属性\n",
    "* 前一层网络神经元个数属性\n",
    "* 当前层神经元个数属性\n",
    "* 最近一次的输入\n",
    "* 最近一次输入对应的加权和\n",
    "* 最近一次的输出\n",
    "\n",
    "方法：\n",
    "\n",
    "* 计算当前层输出方法\n",
    "* 计算当前层梯度方法\n",
    "\n",
    "#### 神经网络类`NeuronNetwork`\n",
    "\n",
    "神经网络类完成神经网络的基本任务：训练与预测。\n",
    "\n",
    "由上一节，若神经网络由n层组成，则需创建n-1个`NeuronLayer`。\n",
    "\n",
    "属性：\n",
    "\n",
    "* 层属性\n",
    "* 输入维度\n",
    "* 输出维度\n",
    "\n",
    "方法：\n",
    "\n",
    "* 训练方法\n",
    "* 预测方法\n",
    "* 计算loss值方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.20141110124953915\n",
      "epoch: 1000, loss: 0.00042012637023430344\n",
      "epoch: 2000, loss: 0.00012870828326731426\n",
      "epoch: 3000, loss: 7.194116399759993e-05\n",
      "epoch: 4000, loss: 5.2051377176518086e-05\n",
      "epoch: 5000, loss: 4.259370775404326e-05\n",
      "epoch: 6000, loss: 3.686633549304985e-05\n",
      "epoch: 7000, loss: 3.251511081617133e-05\n",
      "epoch: 8000, loss: 2.857921728778274e-05\n",
      "epoch: 9000, loss: 2.4679837513360447e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.9923586235573909]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mini_nn.neural_network import NeuralNetwork\n",
    "\n",
    "# 创建神经网络\n",
    "# 隐藏层个数：输入层2，隐藏层3，输出层1\n",
    "# 学习率：0.2\n",
    "# 激活函数：'sigmoid' （默认）,可选值包括'tanh'、'sigmoid'和'linear'（用于输出层）\n",
    "nn = NeuralNetwork([2, 3, 1], 0.2)  \n",
    "# 训练数据（OR），重复一万次，每一千次输出损失值，损失函数暂时只能使用MSE\n",
    "nn.fit([[0, 1],[1, 0], [1, 1], [0, 0]], [[1], [1], [1], [0]], epoch=10000, interval=1000)\n",
    "# 预测\n",
    "nn.predict([[0, 1]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
